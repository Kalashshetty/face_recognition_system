{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM89BcILoh5NypZ6fKPwPDb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalashshetty/face_recognition_system/blob/main/Face_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DxQtTxn0ygL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "\n",
        "# Force TensorFlow to use CPU only\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "def upload_and_extract_dataset():\n",
        "    print(\"Please upload your zipped dataset containing face images\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file was uploaded\")\n",
        "    zip_file = list(uploaded.keys())[0]\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        print(\"Extracting ZIP contents:\")\n",
        "        for file_info in zip_ref.infolist():\n",
        "            print(f\"- {file_info.filename}\")\n",
        "        zip_ref.extractall('dataset')\n",
        "\n",
        "    if not os.path.exists('dataset'):\n",
        "        raise ValueError(\"Dataset extraction failed\")\n",
        "    return 'dataset'\n",
        "\n",
        "def prepare_training_data(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        zoom_range=0.2,\n",
        "        shear_range=0.2\n",
        "    )\n",
        "\n",
        "    base_path = os.path.join(dataset_path, 'ALL_Dataset')\n",
        "    if not os.path.exists(base_path):\n",
        "        base_path = dataset_path  # Fallback to root if ALL_Dataset not found\n",
        "\n",
        "    print(f\"\\nScanning dataset directory: {base_path}\")\n",
        "    for person_name in os.listdir(base_path):\n",
        "        person_path = os.path.join(base_path, person_name)\n",
        "        if os.path.isdir(person_path):\n",
        "            image_count = 0\n",
        "            print(f\"\\nProcessing {person_name}:\")\n",
        "\n",
        "            for img_name in os.listdir(person_path):\n",
        "                img_path = os.path.join(person_path, img_name)\n",
        "                if os.path.isfile(img_path) and img_name.lower().endswith(('.jpg', '.png')):\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        img = cv2.resize(img, (224, 224))\n",
        "                        img = img / 255.0\n",
        "                        images.append(img)\n",
        "                        labels.append(current_label)\n",
        "                        image_count += 1\n",
        "                        print(f\"- Loaded {img_name}\")\n",
        "                    else:\n",
        "                        print(f\"- Failed to load {img_name}\")\n",
        "\n",
        "            print(f\"Loaded {image_count} images for {person_name}\")\n",
        "            if image_count > 0:\n",
        "                label_map[current_label] = person_name\n",
        "                current_label += 1\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"No valid images found in the dataset. See above logs for details.\")\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    print(f\"\\nTotal images loaded: {len(images)}\")\n",
        "    print(f\"Total people: {len(label_map)}\")\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    for img, label in zip(images, labels):\n",
        "        img = np.expand_dims(img, 0)\n",
        "        aug_iter = datagen.flow(img, batch_size=1)\n",
        "        for _ in range(3):\n",
        "            aug_img = next(aug_iter)[0]\n",
        "            augmented_images.append(aug_img)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    images = np.concatenate([images, augmented_images])\n",
        "    labels = np.concatenate([labels, augmented_labels])\n",
        "\n",
        "    return images, labels, label_map\n",
        "\n",
        "def create_and_train_model(images, labels):\n",
        "    if len(images) < 5:\n",
        "        raise ValueError(f\"Not enough samples ({len(images)}) to train the model\")\n",
        "\n",
        "    base_model = applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(set(labels)), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss' if len(images) < 10 else 'val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    validation_split = 0.2 if len(images) >= 10 else 0.0\n",
        "\n",
        "    history = model.fit(\n",
        "        images, labels,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "            await new Promise((resolve) => setTimeout(resolve, 1000));\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = base64.b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        dataset_path = upload_and_extract_dataset()\n",
        "        images, labels, label_map = prepare_training_data(dataset_path)\n",
        "\n",
        "        print(\"Training the model on CPU...\")\n",
        "        with tf.device('/CPU:0'):\n",
        "            model = create_and_train_model(images, labels)\n",
        "\n",
        "        results = []\n",
        "        face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "        print(\"Starting camera capture...\")\n",
        "        for i in range(5):\n",
        "            try:\n",
        "                filename = take_photo(f'capture_{i}.jpg')\n",
        "                img = cv2.imread(filename)\n",
        "\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                detected_person = \"Unknown\"\n",
        "\n",
        "                for (x, y, w, h) in faces:\n",
        "                    face = img[y:y+h, x:x+w]\n",
        "                    face = cv2.resize(face, (224, 224))\n",
        "                    face = np.expand_dims(face/255.0, axis=0)\n",
        "\n",
        "                    with tf.device('/CPU:0'):\n",
        "                        prediction = model.predict(face)\n",
        "                    person_id = np.argmax(prediction)\n",
        "                    confidence = np.max(prediction)\n",
        "\n",
        "                    if confidence > 0.95:\n",
        "                        detected_person = label_map[person_id]\n",
        "\n",
        "                    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                    cv2.putText(img, f\"{detected_person} ({confidence:.2f})\",\n",
        "                                (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
        "                                (0, 255, 0), 2)\n",
        "\n",
        "                cv2.imwrite(f'processed_{i}.jpg', img)\n",
        "                results.append({'timestamp': timestamp, 'person': detected_person})\n",
        "\n",
        "                print(f\"Captured image {i+1}/5 - Detected: {detected_person}\")\n",
        "                if i < 4:\n",
        "                    time.sleep(30)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in capture {i}: {str(e)}\")\n",
        "\n",
        "        df = pd.DataFrame(results)\n",
        "        df_missing = pd.DataFrame(columns=['timestamp', 'missing_person'])\n",
        "\n",
        "        all_people = set(label_map.values())\n",
        "        for timestamp, person in zip(df['timestamp'], df['person']):\n",
        "            if person != \"Unknown\":\n",
        "                missing = all_people - {person}\n",
        "                for m in missing:\n",
        "                    df_missing = pd.concat([df_missing, pd.DataFrame([{\n",
        "                        'timestamp': timestamp,\n",
        "                        'missing_person': m\n",
        "                    }])], ignore_index=True)\n",
        "\n",
        "        with pd.ExcelWriter('detection_results.xlsx') as writer:\n",
        "            df.to_excel(writer, sheet_name='Detections', index=False)\n",
        "            df_missing.to_excel(writer, sheet_name='Missing', index=False)\n",
        "\n",
        "        print(\"Results saved to 'detection_results.xlsx'\")\n",
        "        files.download('detection_results.xlsx')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nDebugging tips:\")\n",
        "        print(\"- Check if images are readable\")\n",
        "        print(\"- Verify sufficient memory available\")\n",
        "        print(\"- Ensure images contain detectable faces\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "\n",
        "# Force TensorFlow to use CPU only\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "def upload_and_extract_dataset():\n",
        "    print(\"Please upload your zipped dataset containing face images\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file was uploaded\")\n",
        "    zip_file = list(uploaded.keys())[0]\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        print(\"Extracting ZIP contents:\")\n",
        "        for file_info in zip_ref.infolist():\n",
        "            print(f\"- {file_info.filename}\")\n",
        "        zip_ref.extractall('dataset')\n",
        "\n",
        "    if not os.path.exists('dataset'):\n",
        "        raise ValueError(\"Dataset extraction failed\")\n",
        "    return 'dataset'\n",
        "\n",
        "def prepare_training_data(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        zoom_range=0.2,\n",
        "        shear_range=0.2\n",
        "    )\n",
        "\n",
        "    base_path = os.path.join(dataset_path, 'ALL_Dataset')\n",
        "    if not os.path.exists(base_path):\n",
        "        base_path = dataset_path  # Fallback to root if ALL_Dataset not found\n",
        "\n",
        "    print(f\"\\nScanning dataset directory: {base_path}\")\n",
        "    for person_name in os.listdir(base_path):\n",
        "        person_path = os.path.join(base_path, person_name)\n",
        "        if os.path.isdir(person_path):\n",
        "            image_count = 0\n",
        "            print(f\"\\nProcessing {person_name}:\")\n",
        "\n",
        "            for img_name in os.listdir(person_path):\n",
        "                img_path = os.path.join(person_path, img_name)\n",
        "                if os.path.isfile(img_path) and img_name.lower().endswith(('.jpg', '.png')):\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        img = cv2.resize(img, (224, 224))\n",
        "                        img = img / 255.0\n",
        "                        images.append(img)\n",
        "                        labels.append(current_label)\n",
        "                        image_count += 1\n",
        "                        print(f\"- Loaded {img_name}\")\n",
        "                    else:\n",
        "                        print(f\"- Failed to load {img_name}\")\n",
        "\n",
        "            print(f\"Loaded {image_count} images for {person_name}\")\n",
        "            if image_count > 0:\n",
        "                label_map[current_label] = person_name\n",
        "                current_label += 1\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"No valid images found in the dataset. See above logs for details.\")\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    print(f\"\\nTotal images loaded: {len(images)}\")\n",
        "    print(f\"Total people: {len(label_map)}\")\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    for img, label in zip(images, labels):\n",
        "        img = np.expand_dims(img, 0)\n",
        "        aug_iter = datagen.flow(img, batch_size=1)\n",
        "        for _ in range(3):\n",
        "            aug_img = next(aug_iter)[0]\n",
        "            augmented_images.append(aug_img)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    images = np.concatenate([images, augmented_images])\n",
        "    labels = np.concatenate([labels, augmented_labels])\n",
        "\n",
        "    return images, labels, label_map\n",
        "\n",
        "def create_and_train_model(images, labels):\n",
        "    if len(images) < 5:\n",
        "        raise ValueError(f\"Not enough samples ({len(images)}) to train the model\")\n",
        "\n",
        "    base_model = applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(set(labels)), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss' if len(images) < 10 else 'val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    validation_split = 0.2 if len(images) >= 10 else 0.0\n",
        "\n",
        "    history = model.fit(\n",
        "        images, labels,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "            await new Promise((resolve) => setTimeout(resolve, 1000));\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = base64.b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        dataset_path = upload_and_extract_dataset()\n",
        "        images, labels, label_map = prepare_training_data(dataset_path)\n",
        "\n",
        "        print(\"Training the model on CPU...\")\n",
        "        with tf.device('/CPU:0'):\n",
        "            model = create_and_train_model(images, labels)\n",
        "\n",
        "        results = []\n",
        "        face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "        print(\"Starting camera capture...\")\n",
        "        for i in range(5):\n",
        "            try:\n",
        "                filename = take_photo(f'capture_{i}.jpg')\n",
        "                img = cv2.imread(filename)\n",
        "\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                detected_person = \"Unknown\"\n",
        "\n",
        "                for (x, y, w, h) in faces:\n",
        "                    face = img[y:y+h, x:x+w]\n",
        "                    face = cv2.resize(face, (224, 224))\n",
        "                    face = np.expand_dims(face/255.0, axis=0)\n",
        "\n",
        "                    with tf.device('/CPU:0'):\n",
        "                        prediction = model.predict(face)\n",
        "                    person_id = np.argmax(prediction)\n",
        "                    confidence = np.max(prediction)\n",
        "\n",
        "                    if confidence > 0.95:\n",
        "                        detected_person = label_map[person_id]\n",
        "\n",
        "                    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                    cv2.putText(img, f\"{detected_person} ({confidence:.2f})\",\n",
        "                                (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
        "                                (0, 255, 0), 2)\n",
        "\n",
        "                cv2.imwrite(f'processed_{i}.jpg', img)\n",
        "                results.append({'timestamp': timestamp, 'person': detected_person})\n",
        "\n",
        "                print(f\"Captured image {i+1}/5 - Detected: {detected_person}\")\n",
        "                if i < 4:\n",
        "                    time.sleep(30)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in capture {i}: {str(e)}\")\n",
        "\n",
        "        df = pd.DataFrame(results)\n",
        "        df_missing = pd.DataFrame(columns=['timestamp', 'missing_person'])\n",
        "\n",
        "        all_people = set(label_map.values())\n",
        "        for timestamp, person in zip(df['timestamp'], df['person']):\n",
        "            if person != \"Unknown\":\n",
        "                missing = all_people - {person}\n",
        "                for m in missing:\n",
        "                    df_missing = pd.concat([df_missing, pd.DataFrame([{\n",
        "                        'timestamp': timestamp,\n",
        "                        'missing_person': m\n",
        "                    }])], ignore_index=True)\n",
        "\n",
        "        with pd.ExcelWriter('detection_results.xlsx') as writer:\n",
        "            df.to_excel(writer, sheet_name='Detections', index=False)\n",
        "            df_missing.to_excel(writer, sheet_name='Missing', index=False)\n",
        "\n",
        "        print(\"Results saved to 'detection_results.xlsx'\")\n",
        "        files.download('detection_results.xlsx')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nDebugging tips:\")\n",
        "        print(\"- Check if images are readable\")\n",
        "        print(\"- Verify sufficient memory available\")\n",
        "        print(\"- Ensure images contain detectable faces\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "WWjQTHj103kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "2hdnsa5RlpEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FGVxSjjllp56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f facenet_keras.h5"
      ],
      "metadata": {
        "id": "mwVLB43ZoXva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1\"\n",
        "output = 'facenet_keras.h5'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "ZFTwmmt3o90o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh facenet_keras.h5"
      ],
      "metadata": {
        "id": "j5c7O226patM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('facenet_keras.h5')"
      ],
      "metadata": {
        "id": "qCKqExI8pc0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install opencv-python opencv-python-headless pandas numpy tensorflow gdown\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import files\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from scipy.spatial.distance import cosine\n",
        "import gdown\n",
        "\n",
        "# Force TensorFlow to use CPU only\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# Load pre-trained FaceNet model\n",
        "def load_facenet_model():\n",
        "    if not os.path.exists('facenet_keras.h5'):\n",
        "        print(\"Downloading FaceNet model...\")\n",
        "        url = \"https://drive.google.com/uc?id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1\"\n",
        "        output = 'facenet_keras.h5'\n",
        "        gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Verify file size\n",
        "    file_size = os.path.getsize('facenet_keras.h5')\n",
        "    if file_size < 90000000:  # File should be ~92.4 MB\n",
        "        raise ValueError(\"Downloaded model file is corrupted or incomplete. Please try again.\")\n",
        "\n",
        "    model = tf.keras.models.load_model('facenet_keras.h5')\n",
        "    return model\n",
        "\n",
        "# Function to extract face embeddings using FaceNet\n",
        "def get_embedding(model, face_pixels):\n",
        "    face_pixels = face_pixels.astype('float32')\n",
        "    mean, std = face_pixels.mean(), face_pixels.std()\n",
        "    face_pixels = (face_pixels - mean) / std\n",
        "    samples = np.expand_dims(face_pixels, axis=0)\n",
        "    embedding = model.predict(samples)\n",
        "    return embedding[0]\n",
        "\n",
        "# Function to unzip the dataset\n",
        "def upload_and_extract_dataset():\n",
        "    print(\"Please upload your zipped dataset containing face images\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file was uploaded\")\n",
        "    zip_file = list(uploaded.keys())[0]\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        print(\"Extracting ZIP contents:\")\n",
        "        for file_info in zip_ref.infolist():\n",
        "            print(f\"- {file_info.filename}\")\n",
        "        zip_ref.extractall('dataset')\n",
        "\n",
        "    if not os.path.exists('dataset'):\n",
        "        raise ValueError(\"Dataset extraction failed\")\n",
        "    return 'dataset'\n",
        "\n",
        "# Function to prepare training data\n",
        "def prepare_training_data(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    print(f\"\\nScanning dataset directory: {dataset_path}\")\n",
        "    for person_name in os.listdir(dataset_path):\n",
        "        person_path = os.path.join(dataset_path, person_name)\n",
        "        if os.path.isdir(person_path):\n",
        "            image_count = 0\n",
        "            print(f\"\\nProcessing {person_name}:\")\n",
        "\n",
        "            for img_name in os.listdir(person_path):\n",
        "                img_path = os.path.join(person_path, img_name)\n",
        "                if os.path.isfile(img_path) and img_name.lower().endswith(('.jpg', '.png')):\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        img = cv2.resize(img, (160, 160))  # FaceNet input size\n",
        "                        images.append(img)\n",
        "                        labels.append(current_label)\n",
        "                        image_count += 1\n",
        "                        print(f\"- Loaded {img_name}\")\n",
        "                    else:\n",
        "                        print(f\"- Failed to load {img_name}\")\n",
        "\n",
        "            print(f\"Loaded {image_count} images for {person_name}\")\n",
        "            if image_count > 0:\n",
        "                label_map[current_label] = person_name\n",
        "                current_label += 1\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"No valid images found in the dataset. See above logs for details.\")\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    print(f\"\\nTotal images loaded: {len(images)}\")\n",
        "    print(f\"Total people: {len(label_map)}\")\n",
        "\n",
        "    return images, labels, label_map\n",
        "\n",
        "# Function to take a photo using the webcam\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "            await new Promise((resolve) => setTimeout(resolve, 1000));\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = base64.b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    try:\n",
        "        # Load pre-trained FaceNet model\n",
        "        print(\"Loading FaceNet model...\")\n",
        "        facenet_model = load_facenet_model()\n",
        "\n",
        "        # Upload and extract dataset\n",
        "        dataset_path = upload_and_extract_dataset()\n",
        "\n",
        "        # Prepare training data\n",
        "        images, labels, label_map = prepare_training_data(dataset_path)\n",
        "\n",
        "        # Extract embeddings for all images in the dataset\n",
        "        print(\"Extracting embeddings from dataset...\")\n",
        "        embeddings = []\n",
        "        for img in images:\n",
        "            embedding = get_embedding(facenet_model, img)\n",
        "            embeddings.append(embedding)\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        normalizer = Normalizer(norm='l2')\n",
        "        embeddings = normalizer.transform(embeddings)\n",
        "\n",
        "        # Start camera capture and recognition\n",
        "        results = []\n",
        "        print(\"Starting camera capture...\")\n",
        "        for i in range(5):\n",
        "            try:\n",
        "                filename = take_photo(f'capture_{i}.jpg')\n",
        "                img = cv2.imread(filename)\n",
        "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img_resized = cv2.resize(img_rgb, (160, 160))\n",
        "\n",
        "                # Extract embedding for the captured face\n",
        "                captured_embedding = get_embedding(facenet_model, img_resized)\n",
        "                captured_embedding = normalizer.transform([captured_embedding])\n",
        "\n",
        "                # Compare with dataset embeddings\n",
        "                min_dist = float('inf')\n",
        "                identity = \"Unknown\"\n",
        "                for idx, emb in enumerate(embeddings):\n",
        "                    dist = cosine(captured_embedding, emb)\n",
        "                    if dist < min_dist and dist < 0.5:  # Threshold for recognition\n",
        "                        min_dist = dist\n",
        "                        identity = label_map[labels[idx]]\n",
        "\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                results.append({'timestamp': timestamp, 'person': identity})\n",
        "\n",
        "                print(f\"Captured image {i+1}/5 - Detected: {identity}\")\n",
        "                if i < 4:\n",
        "                    time.sleep(30)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in capture {i}: {str(e)}\")\n",
        "\n",
        "        # Save results to Excel\n",
        "        df = pd.DataFrame(results)\n",
        "        with pd.ExcelWriter('detection_results.xlsx') as writer:\n",
        "            df.to_excel(writer, sheet_name='Detections', index=False)\n",
        "\n",
        "        print(\"Results saved to 'detection_results.xlsx'\")\n",
        "        files.download('detection_results.xlsx')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        print(\"\\nDebugging tips:\")\n",
        "        print(\"- Check if images are readable\")\n",
        "        print(\"- Verify sufficient memory available\")\n",
        "        print(\"- Ensure images contain detectable faces\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "LclLrUSuWxBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GROK\n"
      ],
      "metadata": {
        "id": "QsrBLHMKsdNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies and build Dlib without CUDA\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential cmake libopenblas-dev liblapack-dev\n",
        "!pip install opencv-python numpy pandas openpyxl\n",
        "!pip uninstall -y dlib  # Remove any existing Dlib\n",
        "!git clone https://github.com/davisking/dlib.git\n",
        "%cd dlib\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake .. -DUSE_AVX_INSTRUCTIONS=1 -DDLIB_USE_CUDA=0  # Explicitly disable CUDA\n",
        "!cmake --build . --config Release\n",
        "!cd .. && python setup.py install --no DLIB_USE_CUDA\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import dlib\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Function to upload and extract zip file\n",
        "def upload_and_extract_dataset():\n",
        "    print(\"Please upload your zipped dataset containing face images (e.g., faces.zip)\")\n",
        "    uploaded = files.upload()\n",
        "    zip_file = list(uploaded.keys())[0]\n",
        "\n",
        "    extract_path = '/content/dataset'\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    return os.path.join(extract_path, 'ALL_Dataset')\n",
        "\n",
        "# Function to get face embedding using Dlib\n",
        "def get_embedding(face_img, detector, shape_predictor, face_recognizer):\n",
        "    rgb_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
        "    dets = detector(rgb_img, 1)\n",
        "    if len(dets) == 0:\n",
        "        return None\n",
        "\n",
        "    d = dets[0]\n",
        "    shape = shape_predictor(rgb_img, d)\n",
        "    embedding = face_recognizer.compute_face_descriptor(rgb_img, shape)\n",
        "    return np.array(embedding)\n",
        "\n",
        "# Function to prepare training data and train KNN\n",
        "def prepare_and_train(dataset_path):\n",
        "    # Initialize Dlib models (CPU-only)\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    shape_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "    face_recognizer = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
        "\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    print(\"Scanning dataset and generating embeddings...\")\n",
        "    for person_name in os.listdir(dataset_path):\n",
        "        person_path = os.path.join(dataset_path, person_name)\n",
        "        if os.path.isdir(person_path):\n",
        "            label_map[current_label] = person_name\n",
        "            image_count = 0\n",
        "            for img_name in os.listdir(person_path):\n",
        "                img_path = os.path.join(person_path, img_name)\n",
        "                if os.path.isfile(img_path):\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        embedding = get_embedding(img, detector, shape_predictor, face_recognizer)\n",
        "                        if embedding is not None:\n",
        "                            embeddings.append(embedding)\n",
        "                            labels.append(current_label)\n",
        "                            image_count += 1\n",
        "            print(f\"Processed {image_count} valid face images for {person_name}\")\n",
        "            current_label += 1\n",
        "\n",
        "    if not embeddings:\n",
        "        raise ValueError(\"No valid face embeddings generated\")\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Normalize embeddings\n",
        "    normalizer = Normalizer(norm='l2')\n",
        "    embeddings_normalized = normalizer.transform(embeddings)\n",
        "\n",
        "    # Train KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
        "    knn.fit(embeddings_normalized, labels)\n",
        "\n",
        "    return knn, normalizer, label_map, detector, shape_predictor, face_recognizer\n",
        "\n",
        "# Function to process uploaded test images\n",
        "def run_recognition(knn, normalizer, label_map, detector, shape_predictor, face_recognizer):\n",
        "    print(\"Please upload test images to recognize faces (e.g., test1.jpg, test2.jpg)\")\n",
        "    uploaded = files.upload()\n",
        "    results = []\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        start_time = time.time()\n",
        "        img = cv2.imdecode(np.frombuffer(uploaded[filename], np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "        dets = detector(img, 1)\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        detected_people = []\n",
        "\n",
        "        print(f\"Detected {len(dets)} faces in {filename}\")\n",
        "        for d in dets:\n",
        "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
        "            left, top = max(0, left), max(0, top)\n",
        "            right, bottom = min(img.shape[1], right), min(img.shape[0], bottom)\n",
        "            face_img = img[top:bottom, left:right]\n",
        "            if face_img.size > 0:\n",
        "                embedding = get_embedding(img, detector, shape_predictor, face_recognizer)\n",
        "                if embedding is not None:\n",
        "                    embedding_normalized = normalizer.transform([embedding])\n",
        "                    prediction = knn.predict(embedding_normalized)\n",
        "                    confidence = knn.predict_proba(embedding_normalized).max()\n",
        "\n",
        "                    person_name = label_map[prediction[0]] if confidence > 0.7 else \"Unknown\"\n",
        "                    detected_people.append(person_name)\n",
        "\n",
        "                    cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0), 2)\n",
        "                    cv2.putText(img, f\"{person_name} ({confidence:.2f})\",\n",
        "                               (left, top-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "        output_filename = f'processed_{filename}'\n",
        "        cv2.imwrite(output_filename, img)\n",
        "        display(Image(filename=output_filename))\n",
        "\n",
        "        result_person = detected_people[0] if detected_people else \"Unknown\"\n",
        "        results.append({'timestamp': timestamp, 'person': result_person})\n",
        "        print(f\"Processed {filename} - Detected: {', '.join(detected_people)} (Time: {time.time() - start_time:.2f}s)\")\n",
        "\n",
        "    # Generate Excel output\n",
        "    df = pd.DataFrame(results)\n",
        "    df_missing = pd.DataFrame(columns=['timestamp', 'missing_person'])\n",
        "\n",
        "    all_people = set(label_map.values())\n",
        "    for timestamp, person in zip(df['timestamp'], df['person']):\n",
        "        if person != \"Unknown\":\n",
        "            missing = all_people - {person}\n",
        "            for m in missing:\n",
        "                df_missing = pd.concat(\n",
        "                    [df_missing, pd.DataFrame([{'timestamp': timestamp, 'missing_person': m}])],\n",
        "                    ignore_index=True\n",
        "                )\n",
        "\n",
        "    output_file = 'detection_results.xlsx'\n",
        "    with pd.ExcelWriter(output_file) as writer:\n",
        "        df.to_excel(writer, sheet_name='Detections', index=False)\n",
        "        df_missing.to_excel(writer, sheet_name='Missing', index=False)\n",
        "\n",
        "    print(f\"Results saved to '{output_file}'\")\n",
        "    files.download(output_file)\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    dataset_path = upload_and_extract_dataset()\n",
        "    print(\"Preparing face embeddings and training classifier...\")\n",
        "    # Download Dlib models if not present\n",
        "    if not os.path.exists('shape_predictor_68_face_landmarks.dat'):\n",
        "        !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "        !bunzip2 shape_predictor_68_face_landmarks.dat.bz2\n",
        "    if not os.path.exists('dlib_face_recognition_resnet_model_v1.dat'):\n",
        "        !wget http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2\n",
        "        !bunzip2 dlib_face_recognition_resnet_model_v1.dat.bz2\n",
        "\n",
        "    knn, normalizer, label_map, detector, shape_predictor, face_recognizer = prepare_and_train(dataset_path)\n",
        "    run_recognition(knn, normalizer, label_map, detector, shape_predictor, face_recognizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bFbAqMcvjbw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GEMINI"
      ],
      "metadata": {
        "id": "wjrNCX6gsYr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['DLIB_USE_CUDA'] = '0'  # Force CPU usage\n",
        "import face_recognition\n",
        "import os, sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def face_confidence(face_distance, face_match_threshold=0.6):\n",
        "    range = (1.0 - face_match_threshold)\n",
        "    linear_val = (1.0 - face_distance) / float(range)\n",
        "\n",
        "    if face_distance > face_match_threshold:\n",
        "        return str(round(linear_val * 100, 2)) + '%'\n",
        "    else:\n",
        "        value = (linear_val + ((1.0 - linear_val) * math.pow((linear_val - 0.5) * 2, 0.2))) * 100\n",
        "        return str(round(value, 2)) + '%'\n",
        "\n",
        "class FaceRecognition:\n",
        "    face_locations = []\n",
        "    face_encodings = []\n",
        "    face_names = []\n",
        "    known_face_encodings = []\n",
        "    known_face_names = []\n",
        "    process_current_frame = True\n",
        "\n",
        "    def __init__(self, faces_dir='faces'):\n",
        "        self.faces_dir = faces_dir\n",
        "        self.encode_faces()\n",
        "\n",
        "    def encode_faces(self):\n",
        "        for image in os.listdir(self.faces_dir):\n",
        "            face_image = face_recognition.load_image_file(os.path.join(self.faces_dir, image))\n",
        "            face_encoding = face_recognition.face_encodings(face_image)\n",
        "            if len(face_encoding) > 0:\n",
        "                self.known_face_encodings.append(face_encoding[0])\n",
        "                self.known_face_names.append(image)\n",
        "            else:\n",
        "                print(f\"Warning: No face found in {image}\")\n",
        "\n",
        "        print(self.known_face_names)\n",
        "\n",
        "    def run_recognition(self):\n",
        "        video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "        if not video_capture.isOpened():\n",
        "            sys.exit('Video source not found...')\n",
        "\n",
        "        while True:\n",
        "            ret, frame = video_capture.read()\n",
        "\n",
        "            if self.process_current_frame:\n",
        "                small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
        "\n",
        "                rgb_small_frame = small_frame[:, :, ::-1]\n",
        "\n",
        "                self.face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "                self.face_encodings = face_recognition.face_encodings(rgb_small_frame, self.face_locations)\n",
        "\n",
        "                self.face_names = []\n",
        "                for face_encoding in self.face_encodings:\n",
        "                    matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
        "                    name = 'Unknown'\n",
        "                    confidence = 'Unknown'\n",
        "\n",
        "                    face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
        "\n",
        "                    best_match_index = np.argmin(face_distances)\n",
        "                    if matches[best_match_index]:\n",
        "                        name = self.known_face_names[best_match_index]\n",
        "                        confidence = face_confidence(face_distances[best_match_index])\n",
        "\n",
        "                    self.face_names.append(f'{name} ({confidence})')\n",
        "\n",
        "            self.process_current_frame = not self.process_current_frame\n",
        "\n",
        "            for (top, right, bottom, left), name in zip(self.face_locations, self.face_names):\n",
        "                top *= 4\n",
        "                right *= 4\n",
        "                bottom *= 4\n",
        "                left *= 4\n",
        "\n",
        "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), -1)\n",
        "                cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
        "\n",
        "            cv2_imshow(frame)\n",
        "\n",
        "            if cv2.waitKey(1) == ord('q'):\n",
        "                break\n",
        "\n",
        "        video_capture.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    uploaded = files.upload()\n",
        "    os.makedirs('faces', exist_ok=True)\n",
        "    for filename in uploaded.keys():\n",
        "        with open(f'faces/{filename}', 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "\n",
        "    fr = FaceRecognition()\n",
        "    fr.run_recognition()"
      ],
      "metadata": {
        "id": "3upKljTSqnAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import face_recognition\n",
        "import os\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Create faces directory if it doesn't exist\n",
        "if not os.path.exists('faces'):\n",
        "    os.makedirs('faces')\n",
        "\n",
        "# Function to calculate face confidence\n",
        "def face_confidence(face_distance, face_match_threshold=0.6):\n",
        "    range_val = 1.0 - face_match_threshold\n",
        "    linear_val = (1.0 - face_distance) / (range_val * 2.0)\n",
        "\n",
        "    if face_distance > face_match_threshold:\n",
        "        return str(round(linear_val * 100, 2)) + '%'\n",
        "    else:\n",
        "        value = (linear_val + ((1.0 - linear_val) * math.pow((linear_val - 0.5) * 2, 0.2))) * 100\n",
        "        return str(round(value, 2)) + '%'\n",
        "\n",
        "class FaceRecognition:\n",
        "    face_locations = []\n",
        "    face_encodings = []\n",
        "    face_names = []\n",
        "    known_face_encodings = []\n",
        "    known_face_names = []\n",
        "    process_current_frame = True\n",
        "\n",
        "    def __init__(self):\n",
        "        self.encode_faces()\n",
        "\n",
        "    def encode_faces(self):\n",
        "        for image in os.listdir('faces'):\n",
        "            face_image = face_recognition.load_image_file(f'faces/{image}')\n",
        "            face_encoding = face_recognition.face_encodings(face_image)\n",
        "            if face_encoding:\n",
        "                self.known_face_encodings.append(face_encoding[0])\n",
        "                self.known_face_names.append(image.split('.')[0])  # Use filename without extension as name\n",
        "            else:\n",
        "                print(f\"Warning: No face found in {image}\")\n",
        "\n",
        "        print(\"Known faces:\", self.known_face_names)\n",
        "\n",
        "    def run_recognition(self):\n",
        "        video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "        if not video_capture.isOpened():\n",
        "            print(\"Video source not found...\") #prints a message instead of sys.exit.\n",
        "            return #exits the function instead of sys.exit\n",
        "\n",
        "        while True:\n",
        "            ret, frame = video_capture.read()\n",
        "\n",
        "            if not ret:\n",
        "                print(\"Failed to capture frame\")\n",
        "                break\n",
        "\n",
        "            if self.process_current_frame:\n",
        "                small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
        "                rgb_small_frame = small_frame[:, :, ::-1]\n",
        "\n",
        "                self.face_locations = face_recognition.face_locations(rgb_small_frame)\n",
        "                self.face_encodings = face_recognition.face_encodings(rgb_small_frame, self.face_locations)\n",
        "\n",
        "                self.face_names = []\n",
        "                for face_encoding in self.face_encodings:\n",
        "                    matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
        "                    name = 'Unknown'\n",
        "                    confidence = 'Unknown'\n",
        "\n",
        "                    face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
        "                    best_match_index = np.argmin(face_distances)\n",
        "                    if matches[best_match_index]:\n",
        "                        name = self.known_face_names[best_match_index]\n",
        "                        confidence = face_confidence(face_distances[best_match_index])\n",
        "\n",
        "                    self.face_names.append(f'{name} ({confidence})')\n",
        "\n",
        "                self.process_current_frame = not self.process_current_frame\n",
        "\n",
        "            for (top, right, bottom, left), name in zip(self.face_locations, self.face_names):\n",
        "                top *= 4\n",
        "                right *= 4\n",
        "                bottom *= 4\n",
        "                left *= 4\n",
        "\n",
        "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
        "                cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
        "\n",
        "            cv2_imshow(frame)\n",
        "\n",
        "            if cv2.waitKey(1) == ord('q'):\n",
        "                break\n",
        "\n",
        "        video_capture.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    fr = FaceRecognition()\n",
        "    fr.run_recognition()"
      ],
      "metadata": {
        "id": "3rVKLCVEx2fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition\n"
      ],
      "metadata": {
        "id": "Sh3ZniIu0JIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (CPU-only or GPU with CUDA)\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    !pip install opencv-python pandas openpyxl pillow torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "    print(\"CUDA available, using GPU acceleration.\")\n",
        "    device = torch.device(\"cuda:0\")\n",
        "except ImportError:\n",
        "    !pip install opencv-python pandas openpyxl pillow\n",
        "    print(\"CUDA not available, using CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "import cv2\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import time\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Upload and extract the zipped dataset\n",
        "display(HTML(\"<h2 style='color: #1E88E5; font-family: Arial;'>Upload Your Dataset</h2>\"))\n",
        "display(HTML(\"<p style='font-family: Arial;'>Please upload a zip file with subfolders of known faces (e.g., dataset/person1/img1.jpg).</p>\"))\n",
        "uploaded = files.upload()\n",
        "extracted_folder = \"dataset\"\n",
        "zip_file_name = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "display(HTML(f\"<p style='color: #388E3C; font-family: Arial;'>Extracted contents: {os.listdir(extracted_folder)}</p>\"))\n",
        "\n",
        "# Step 2: Load Haar Cascades for frontal and profile faces (improved detection)\n",
        "cascade_frontal = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        "cascade_profile = cv2.data.haarcascades + \"haarcascade_profileface.xml\"\n",
        "face_cascade_frontal = cv2.CascadeClassifier(cascade_frontal)\n",
        "face_cascade_profile = cv2.CascadeClassifier(cascade_profile)\n",
        "if face_cascade_frontal.empty() or face_cascade_profile.empty():\n",
        "    raise ValueError(\"Error: Failed to load Haar cascade files.\")\n",
        "\n",
        "# Step 3: Prepare a CNN Model and train with provided dataset (Enhanced Accuracy)\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "class FaceRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FaceRecognitionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 25 * 25, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "img_paths = []\n",
        "labels = []\n",
        "label_dict = {}\n",
        "current_label = 0\n",
        "\n",
        "for root, _, file_list in os.walk(extracted_folder):\n",
        "    person_name = Path(root).name\n",
        "    if not file_list or person_name == extracted_folder:\n",
        "        continue\n",
        "    for filename in file_list:\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_path = os.path.join(root, filename)\n",
        "            img_paths.append(img_path)\n",
        "            if person_name not in label_dict:\n",
        "                label_dict[current_label] = person_name\n",
        "                current_label += 1\n",
        "            label_id = [k for k, v in label_dict.items() if v == person_name][0]\n",
        "            labels.append(label_id)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = FaceDataset(img_paths, labels, transform=transform)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = FaceRecognitionModel(len(label_dict)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for images, label_ids in train_loader:\n",
        "        images, label_ids = images.to(device), label_ids.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, label_ids)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "display(HTML(f\"<p style='color: #388E3C; font-family: Arial;'>Trained with {len(img_paths)} faces from {len(label_dict)} persons: {label_dict}</p>\"))\n",
        "\n",
        "# Step 4: Webcam function with automatic capture (same as before)\n",
        "def capture_frame(filename='photo.jpg', quality=0.8):\n",
        "    # ... (same as before)\n",
        "    js = Javascript('''\n",
        "    async function captureFrame(quality) {\n",
        "        // ... (same as before)\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('captureFrame({})'.format(quality))\n",
        "    if data is None:\n",
        "        return None\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Step 5: Process frames and log timestamps for multiple faces (Enhanced Accuracy)\n",
        "def process_frame(img_path, capture_num, log_data):\n",
        "    frame = cv2.imread(img_path)\n",
        "    if frame is None:\n",
        "        display(HTML(\"<p style='color: #F44336; font-family: Arial;'>Error: Failed to load image.</p>\"))\n",
        "        return None\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frontal_faces = face_cascade_frontal.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
        "    profile_faces = face_cascade_profile.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
        "    faces_detected = np.vstack((frontal_faces, profile_faces)) if len(frontal_faces) > 0 and len(profile_faces) > 0 else frontal_faces if len(frontal_faces) > 0 else profile_faces\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    entries = []\n",
        "    print(f\"Capture {capture_num + 1}: Detected {len(faces_detected)} faces\")\n",
        "    detected_names = set()"
      ],
      "metadata": {
        "id": "ZIAdtbSJ-byt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision"
      ],
      "metadata": {
        "id": "d37dRr85Oknn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "Lh6ZgOPLPgYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "eqkZgTWGPjSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "dnKRVrs8PmWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (CPU-only or GPU with CUDA)\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    !pip install opencv-python pandas openpyxl pillow torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "    print(\"CUDA available, using GPU acceleration.\")\n",
        "    device = torch.device(\"cuda:0\")\n",
        "except ImportError:\n",
        "    !pip install opencv-python pandas openpyxl pillow\n",
        "    print(\"CUDA not available, using CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "import cv2\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import time\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Upload and extract the zipped dataset\n",
        "display(HTML(\"<h2 style='color: #1E88E5; font-family: Arial;'>Upload Your Dataset</h2>\"))\n",
        "display(HTML(\"<p style='font-family: Arial;'>Please upload a zip file with subfolders of known faces (e.g., dataset/person1/img1.jpg).</p>\"))\n",
        "uploaded = files.upload()\n",
        "extracted_folder = \"dataset\"\n",
        "zip_file_name = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "display(HTML(f\"<p style='color: #388E3C; font-family: Arial;'>Extracted contents: {os.listdir(extracted_folder)}</p>\"))\n",
        "\n",
        "# Step 2: Load Haar Cascades for frontal and profile faces (improved detection)\n",
        "cascade_frontal = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        "cascade_profile = cv2.data.haarcascades + \"haarcascade_profileface.xml\"\n",
        "face_cascade_frontal = cv2.CascadeClassifier(cascade_frontal)\n",
        "face_cascade_profile = cv2.CascadeClassifier(cascade_profile)\n",
        "if face_cascade_frontal.empty() or face_cascade_profile.empty():\n",
        "    raise ValueError(\"Error: Failed to load Haar cascade files.\")\n",
        "\n",
        "# Step 3: Prepare a CNN Model and train with provided dataset (Enhanced Accuracy)\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "class FaceRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FaceRecognitionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 25 * 25, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "img_paths = []\n",
        "labels = []\n",
        "label_dict = {}\n",
        "current_label = 0\n",
        "\n",
        "for root, _, file_list in os.walk(extracted_folder):\n",
        "    person_name = Path(root).name\n",
        "    if not file_list or person_name == extracted_folder:\n",
        "        continue\n",
        "    for filename in file_list:\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_path = os.path.join(root, filename)\n",
        "            img_paths.append(img_path)\n",
        "            if person_name not in label_dict:\n",
        "                label_dict[current_label] = person_name\n",
        "                current_label += 1\n",
        "            label_id = [k for k, v in label_dict.items() if v == person_name][0]\n",
        "            labels.append(label_id)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = FaceDataset(img_paths, labels, transform=transform)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = FaceRecognitionModel(len(label_dict)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for images, label_ids in train_loader:\n",
        "        images, label_ids = images.to(device), label_ids.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, label_ids)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "display(HTML(f\"<p style='color: #388E3C; font-family: Arial;'>Trained with {len(img_paths)} faces from {len(label_dict)} persons: {label_dict}</p>\"))\n",
        "\n",
        "# Step 4: Webcam function with automatic capture (same as before)\n",
        "def capture_frame(filename='photo.jpg', quality=0.8):\n",
        "    # ... (same as before)\n",
        "    js = Javascript('''\n",
        "    async function captureFrame(quality) {\n",
        "        // ... (same as before)\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('captureFrame({})'.format(quality))\n",
        "    if data is None:\n",
        "        return None\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Step 5: Process frames and log timestamps for multiple faces (Enhanced Accuracy)\n",
        "def process_frame(img_path, capture_num, log_data):\n",
        "    frame = cv2.imread(img_path)\n",
        "    if frame is None:\n",
        "        display(HTML(\"<p style='color: #F44336; font-family: Arial;'>Error: Failed to load image.</p>\"))\n",
        "        return None\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frontal_faces = face_cascade_frontal.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
        "    profile_faces = face_cascade_profile.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
        "    faces_detected = np.vstack((frontal_faces, profile_faces)) if len(frontal_faces) > 0 and len(profile_faces) > 0 else frontal_faces if len(frontal_faces) > 0 else profile_faces\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    entries = []\n",
        "    print(f\"Capture {capture_num + 1}: Detected {len(faces_detected)} faces\")\n",
        "    detected_names = set()\n"
      ],
      "metadata": {
        "id": "1T5sGwokPpYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Webcam function with automatic capture\n",
        "def capture_frame(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function captureFrame(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Resize the output to fit the video element.\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "        // Wait for Capture to be clicked.\n",
        "        await new Promise((resolve) => setTimeout(resolve, 30000)); // Wait for 30 seconds\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        div.remove();\n",
        "        return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('captureFrame({})'.format(quality))\n",
        "    if data is None:\n",
        "        return None\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Step 5: Process frames and log timestamps for multiple faces (Enhanced Accuracy)\n",
        "def process_frame(img_path, capture_num, log_data):\n",
        "    frame = cv2.imread(img_path)\n",
        "    if frame is None:\n",
        "        display(HTML(\"<p style='color: #F44336; font-family: Arial;'>Error: Failed to load image.</p>\"))\n",
        "        return None\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frontal_faces = face_cascade_frontal.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
        "    profile_faces = face_cascade_profile.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
        "    faces_detected = np.vstack((frontal_faces, profile_faces)) if len(frontal_faces) > 0 and len(profile_faces) > 0 else frontal_faces if len(frontal_faces) > 0 else profile_faces\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    entries = []\n",
        "    print(f\"Capture {capture_num + 1}: Detected {len(faces_detected)} faces\")\n",
        "    detected_names = set()\n",
        "\n",
        "    for i, (x, y, w, h) in enumerate(faces_detected):\n",
        "        face_img = frame[y:y+h, x:x+w]\n",
        "        face_img = cv2.resize(face_img, (100, 100))\n",
        "        face_img = Image.fromarray(face_img).convert('RGB')\n",
        "        face_img = transform(face_img).unsqueeze(0).to(device)\n",
        "        outputs = model(face_img)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predicted_label = label_dict[predicted.item()]\n",
        "        detected_names.add(predicted_label)\n",
        "        entries.append([timestamp, predicted_label, x, y, w, h])\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    log_data.extend(entries)\n",
        "    display(HTML(f\"<p style='color: #1E88E5; font-family: Arial;'>Capture {capture_num + 1}: Recognized {', '.join(detected_names)}</p>\"))\n",
        "    cv2.imwrite(f\"capture_{capture_num + 1}.jpg\", frame)\n",
        "    return frame\n",
        "\n",
        "# Step 6: Automatically capture and process frames at 30-second intervals\n",
        "log_data = []\n",
        "for i in range(4):\n",
        "    img_path = capture_frame(filename=f'capture_{i + 1}.jpg')\n",
        "    if img_path:\n",
        "        process_frame(img_path, i, log_data)\n",
        "    time.sleep(30)  # Wait for 30 seconds before the next capture\n",
        "\n",
        "# Step 7: Save the log data to an Excel file\n",
        "log_df = pd.DataFrame(log_data, columns=[\"Timestamp\", \"Name\", \"X\", \"Y\", \"Width\", \"Height\"])\n",
        "log_df.to_excel(\"face_recognition_log.xlsx\", index=False)\n",
        "display(HTML(\"<p style='color: #388E3C; font-family: Arial;'>Face recognition log saved to 'face_recognition_log.xlsx'.</p>\"))"
      ],
      "metadata": {
        "id": "u9dBMa-6SqGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O deploy.prototxt https://github.com/opencv/opencv/blob/master/samples/dnn/face_detector/deploy.prototxt\n",
        "!wget -O res10_300x300_ssd_iter_140000.caffemodel https://github.com/opencv/opencv/blob/master/samples/dnn/face_detector/res10_300x300_ssd_iter_140000.caffemodel\n"
      ],
      "metadata": {
        "id": "Rq6SNbilpOcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n"
      ],
      "metadata": {
        "id": "pzJlhbVapSBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O deploy.prototxt https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n",
        "!wget -O res10_300x300_ssd_iter_140000.caffemodel https://raw.githubusercontent.com/opencv/opencv_3rdparty/master/models/dnn/face_detector/res10_300x300_ssd_iter_140000.caffemodel\n"
      ],
      "metadata": {
        "id": "MBYQoO1fqCC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O res10_300x300_ssd_iter_140000.caffemodel https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\n"
      ],
      "metadata": {
        "id": "31yKfm2QqNHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prototxt_path = \"deploy.prototxt\"\n",
        "caffemodel_path = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "\n",
        "face_net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n"
      ],
      "metadata": {
        "id": "TkvG3pa3qQV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O deploy.prototxt https://github.com/opencv/opencv/raw/master/samples/dnn/face_detector/deploy.prototxt\n",
        "!wget -O res10_300x300_ssd_iter_140000.caffemodel https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\n"
      ],
      "metadata": {
        "id": "eFJy92IqqbWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh res10_300x300_ssd_iter_140000.caffemodel\n"
      ],
      "metadata": {
        "id": "o01TEonVqe_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "prototxt_path = \"deploy.prototxt\"\n",
        "caffemodel_path = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "\n",
        "face_net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "69-lrb-TqgL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O yunet.onnx https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\n"
      ],
      "metadata": {
        "id": "RRq1BbhPquE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_net = cv2.dnn.readNet(\"yunet.onnx\")\n"
      ],
      "metadata": {
        "id": "2JYQwpnLqvg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import time\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Install dependencies (CUDA support if available)\n",
        "try:\n",
        "    !pip install opencv-python pandas openpyxl pillow torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "except ImportError:\n",
        "    !pip install opencv-python pandas openpyxl pillow\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU.\")\n",
        "\n",
        "# Step 1: Upload and extract the dataset\n",
        "display(HTML(\"<h2 style='color: #1E88E5;'>Upload Your Dataset</h2>\"))\n",
        "uploaded = files.upload()\n",
        "extracted_folder = \"dataset\"\n",
        "zip_file_name = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "display(HTML(f\"<p style='color: #388E3C;'>Extracted: {os.listdir(extracted_folder)}</p>\"))\n",
        "\n",
        "# Load DNN-based face detection model\n",
        "prototxt_path = cv2.data.haarcascades + \"deploy.prototxt\"\n",
        "caffemodel_path = cv2.data.haarcascades + \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "face_net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
        "\n",
        "# Step 2: Prepare Pretrained ResNet for Face Recognition\n",
        "class FaceRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FaceRecognitionModel, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Step 3: Load dataset and prepare training\n",
        "def load_dataset():\n",
        "    img_paths, labels, label_dict = [], [], {}\n",
        "    label_index = 0\n",
        "\n",
        "    for root, _, file_list in os.walk(extracted_folder):\n",
        "        person_name = Path(root).name\n",
        "        if not file_list or person_name == extracted_folder:\n",
        "            continue\n",
        "        for filename in file_list:\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                img_paths.append(os.path.join(root, filename))\n",
        "                if person_name not in label_dict:\n",
        "                    label_dict[label_index] = person_name\n",
        "                    label_index += 1\n",
        "                labels.append([k for k, v in label_dict.items() if v == person_name][0])\n",
        "\n",
        "    return img_paths, labels, label_dict\n",
        "\n",
        "img_paths, labels, label_dict = load_dataset()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class FaceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_paths, labels, transform):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "        return self.transform(img), self.labels[idx]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(FaceDataset(img_paths, labels, transform), batch_size=32, shuffle=True)\n",
        "\n",
        "# Train the model\n",
        "model = FaceRecognitionModel(len(label_dict)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for images, label_ids in train_loader:\n",
        "        images, label_ids = images.to(device), label_ids.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, label_ids)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/10, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Step 4: Webcam function for automatic capture\n",
        "def capture_frame(filename='photo.jpg'):\n",
        "    js = Javascript('''\n",
        "    async function captureFrame() {\n",
        "        const video = document.createElement('video');\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        document.body.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "        await new Promise((resolve) => setTimeout(resolve, 10000));\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        return canvas.toDataURL('image/jpeg');\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('captureFrame()')\n",
        "    if data is None:\n",
        "        return None\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(b64decode(data.split(',')[1]))\n",
        "    return filename\n",
        "\n",
        "# Step 5: Face detection and recognition\n",
        "def process_frame(img_path):\n",
        "    frame = cv2.imread(img_path)\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    face_net.setInput(blob)\n",
        "    detections = face_net.forward()\n",
        "    detected_faces = []\n",
        "\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
        "            (x, y, x2, y2) = box.astype('int')\n",
        "            detected_faces.append((x, y, x2-x, y2-y))\n",
        "            face_img = frame[y:y2, x:x2]\n",
        "            face_img = transform(Image.fromarray(face_img).convert('RGB')).unsqueeze(0).to(device)\n",
        "            _, predicted = torch.max(model(face_img), 1)\n",
        "            predicted_label = label_dict[predicted.item()]\n",
        "            cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, predicted_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    cv2.imwrite(\"processed.jpg\", frame)\n",
        "    return frame\n",
        "\n",
        "# Step 6: Capture and Process\n",
        "detected_faces = process_frame(capture_frame())\n",
        "display(HTML(\"<p style='color: #388E3C;'>Processed Image Saved.</p>\"))\n"
      ],
      "metadata": {
        "id": "wE914MfpVFsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition opencv-python pandas openpyxl\n"
      ],
      "metadata": {
        "id": "eE1cQTGkZukA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pytesseract\n",
        "from transformers import MistralForCausalLM, AutoTokenizer\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DoctorHandwritingOCR:\n",
        "    def __init__(self, data_dir=\"./data\", output_dir=\"./output\", model_save_path=\"./models\"):\n",
        "        \"\"\"\n",
        "        Initialize the Doctor's Handwriting OCR system.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory containing the training data\n",
        "            output_dir: Directory to store output files\n",
        "            model_save_path: Directory to save trained models\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.model_save_path = model_save_path\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for directory in [data_dir, output_dir, model_save_path]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        # Initialize Mistral model for language improvement\n",
        "        self.tokenizer = None\n",
        "        self.mistral_model = None\n",
        "        self.ocr_model = None\n",
        "        self.image_size = (224, 224)\n",
        "        self.batch_size = 16\n",
        "\n",
        "        # Set path for tesseract (update this to your installation path)\n",
        "        pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "    def load_mistral_model(self):\n",
        "        \"\"\"Load the Mistral language model for refining OCR results.\"\"\"\n",
        "        logger.info(\"Loading Mistral model for text refinement...\")\n",
        "        try:\n",
        "            model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.mistral_model = MistralForCausalLM.from_pretrained(model_name)\n",
        "            logger.info(\"Mistral model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading Mistral model: {str(e)}\")\n",
        "            logger.info(\"Continuing without Mistral refinement.\")\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\"\n",
        "        Preprocess an image for OCR.\n",
        "\n",
        "        Args:\n",
        "            image_path: Path to the image file\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed image ready for OCR\n",
        "        \"\"\"\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            logger.error(f\"Failed to load image: {image_path}\")\n",
        "            return None\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply thresholding to handle different lighting conditions\n",
        "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "        # Noise removal\n",
        "        kernel = np.ones((2, 2), np.uint8)\n",
        "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "\n",
        "        # Dilation to connect nearby text\n",
        "        dilation = cv2.dilate(opening, kernel, iterations=1)\n",
        "\n",
        "        # Resize image to standardized size\n",
        "        processed = cv2.resize(dilation, self.image_size)\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def extract_text_with_tesseract(self, image):\n",
        "        \"\"\"\n",
        "        Extract text from an image using Tesseract OCR.\n",
        "\n",
        "        Args:\n",
        "            image: Preprocessed image\n",
        "\n",
        "        Returns:\n",
        "            Extracted text\n",
        "        \"\"\"\n",
        "        # Convert back to PIL Image for Tesseract\n",
        "        pil_image = Image.fromarray(image)\n",
        "\n",
        "        # Use Tesseract OCR with medical dictionary\n",
        "        custom_config = r'--oem 3 --psm 6 -l eng+med'\n",
        "        text = pytesseract.image_to_string(pil_image, config=custom_config)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def refine_text_with_mistral(self, raw_text):\n",
        "        \"\"\"\n",
        "        Refine the OCR-extracted text using the Mistral language model.\n",
        "\n",
        "        Args:\n",
        "            raw_text: Raw text from OCR\n",
        "\n",
        "        Returns:\n",
        "            Refined text\n",
        "        \"\"\"\n",
        "        if self.mistral_model is None or self.tokenizer is None:\n",
        "            return raw_text\n",
        "\n",
        "        try:\n",
        "            prompt = f\"Correct this medical text that was extracted from a doctor's handwriting: '{raw_text}'\"\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "            # Generate improved text\n",
        "            outputs = self.mistral_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=256,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2\n",
        "            )\n",
        "\n",
        "            refined_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract the correction part\n",
        "            match = re.search(r\"doctor's handwriting:(.*)\", refined_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "            return refined_text.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error refining text with Mistral: {str(e)}\")\n",
        "            return raw_text\n",
        "\n",
        "    def create_dataset(self, annotations_file):\n",
        "        \"\"\"\n",
        "        Create a dataset from images and their ground truth annotations.\n",
        "\n",
        "        Args:\n",
        "            annotations_file: CSV file containing image paths and ground truth text\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with image paths, processed images, OCR text, and ground truth\n",
        "        \"\"\"\n",
        "        logger.info(\"Creating dataset from annotations...\")\n",
        "\n",
        "        # Read annotations\n",
        "        df = pd.read_csv(annotations_file)\n",
        "        dataset = []\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            image_path = os.path.join(self.data_dir, row['image_path'])\n",
        "            ground_truth = row['text']\n",
        "\n",
        "            # Preprocess image\n",
        "            processed_image = self.preprocess_image(image_path)\n",
        "            if processed_image is None:\n",
        "                continue\n",
        "\n",
        "            # Extract text with OCR\n",
        "            ocr_text = self.extract_text_with_tesseract(processed_image)\n",
        "\n",
        "            dataset.append({\n",
        "                'image_path': image_path,\n",
        "                'processed_image': processed_image,\n",
        "                'ocr_text': ocr_text,\n",
        "                'ground_truth': ground_truth\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(dataset)\n",
        "\n",
        "    def build_enhancement_model(self):\n",
        "        \"\"\"\n",
        "        Build a deep learning model to enhance OCR results.\n",
        "\n",
        "        Returns:\n",
        "            Compiled model\n",
        "        \"\"\"\n",
        "        logger.info(\"Building OCR enhancement model...\")\n",
        "\n",
        "        # Use MobileNetV2 as base model\n",
        "        base_model = MobileNetV2(\n",
        "            input_shape=(self.image_size[0], self.image_size[1], 3),\n",
        "            include_top=False,\n",
        "            weights='imagenet'\n",
        "        )\n",
        "\n",
        "        # Freeze base model layers\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Create model\n",
        "        model = models.Sequential([\n",
        "            base_model,\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(1024, activation='relu'),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.Dense(128, activation='relu')\n",
        "        ])\n",
        "\n",
        "        # Add LSTM layers for sequence modeling\n",
        "        model.add(layers.Reshape((1, 128)))\n",
        "        model.add(layers.LSTM(128, return_sequences=True))\n",
        "        model.add(layers.LSTM(64))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(layers.Dense(1000, activation='softmax'))  # Assuming vocabulary size of 1000\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, dataset, epochs=30):\n",
        "        \"\"\"\n",
        "        Train the OCR enhancement model.\n",
        "\n",
        "        Args:\n",
        "            dataset: DataFrame containing training data\n",
        "            epochs: Number of training epochs\n",
        "\n",
        "        Returns:\n",
        "            Trained model\n",
        "        \"\"\"\n",
        "        logger.info(\"Training OCR enhancement model...\")\n",
        "\n",
        "        # Split dataset into training and validation sets\n",
        "        train_df, val_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Build model\n",
        "        model = self.build_enhancement_model()\n",
        "\n",
        "        # Data generators\n",
        "        # Note: In a real implementation, you'd need to convert text to sequences\n",
        "        # and create proper data generators\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping = callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        # Model checkpoint\n",
        "        checkpoint = callbacks.ModelCheckpoint(\n",
        "            os.path.join(self.model_save_path, 'best_model.h5'),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "        # Train model (placeholder - actual training would use data generators)\n",
        "        logger.info(\"Model architecture created. In a real implementation, you would train with actual data.\")\n",
        "\n",
        "        # Save model\n",
        "        model.save(os.path.join(self.model_save_path, 'final_model.h5'))\n",
        "\n",
        "        self.ocr_model = model\n",
        "        return model\n",
        "\n",
        "    def recognize_text(self, image_path):\n",
        "        \"\"\"\n",
        "        Recognize text from a doctor's handwriting image.\n",
        "\n",
        "        Args:\n",
        "            image_path: Path to the image file\n",
        "\n",
        "        Returns:\n",
        "            Recognized text\n",
        "        \"\"\"\n",
        "        logger.info(f\"Recognizing text from image: {image_path}\")\n",
        "\n",
        "        # Preprocess image\n",
        "        processed_image = self.preprocess_image(image_path)\n",
        "        if processed_image is None:\n",
        "            return \"Error: Could not process image.\"\n",
        "\n",
        "        # Extract text with OCR\n",
        "        ocr_text = self.extract_text_with_tesseract(processed_image)\n",
        "        logger.info(f\"Raw OCR text: {ocr_text}\")\n",
        "\n",
        "        # Refine text with Mistral\n",
        "        refined_text = self.refine_text_with_mistral(ocr_text)\n",
        "        logger.info(f\"Refined text: {refined_text}\")\n",
        "\n",
        "        return {\n",
        "            'raw_ocr': ocr_text,\n",
        "            'refined_text': refined_text\n",
        "        }\n",
        "\n",
        "    def evaluate_model(self, test_dataset):\n",
        "        \"\"\"\n",
        "        Evaluate the model on test data.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: DataFrame containing test data\n",
        "\n",
        "        Returns:\n",
        "            Evaluation metrics\n",
        "        \"\"\"\n",
        "        logger.info(\"Evaluating model performance...\")\n",
        "\n",
        "        results = []\n",
        "        for _, row in tqdm(test_dataset.iterrows(), total=len(test_dataset)):\n",
        "            # Extract raw OCR text\n",
        "            ocr_text = row['ocr_text']\n",
        "\n",
        "            # Refine text with Mistral\n",
        "            refined_text = self.refine_text_with_mistral(ocr_text)\n",
        "\n",
        "            # Compare with ground truth\n",
        "            ground_truth = row['ground_truth']\n",
        "\n",
        "            # Calculate metrics\n",
        "            raw_accuracy = self.calculate_text_similarity(ocr_text, ground_truth)\n",
        "            refined_accuracy = self.calculate_text_similarity(refined_text, ground_truth)\n",
        "\n",
        "            results.append({\n",
        "                'image_path': row['image_path'],\n",
        "                'raw_ocr': ocr_text,\n",
        "                'refined_text': refined_text,\n",
        "                'ground_truth': ground_truth,\n",
        "                'raw_accuracy': raw_accuracy,\n",
        "                'refined_accuracy': refined_accuracy\n",
        "            })\n",
        "\n",
        "        # Save results\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv(os.path.join(self.output_dir, 'evaluation_results.csv'), index=False)\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_raw_accuracy = results_df['raw_accuracy'].mean()\n",
        "        overall_refined_accuracy = results_df['refined_accuracy'].mean()\n",
        "\n",
        "        logger.info(f\"Overall raw OCR accuracy: {overall_raw_accuracy:.4f}\")\n",
        "        logger.info(f\"Overall refined text accuracy: {overall_refined_accuracy:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'raw_accuracy': overall_raw_accuracy,\n",
        "            'refined_accuracy': overall_refined_accuracy,\n",
        "            'detailed_results': results_df\n",
        "        }\n",
        "\n",
        "    def calculate_text_similarity(self, text1, text2):\n",
        "        \"\"\"\n",
        "        Calculate similarity between two texts.\n",
        "\n",
        "        Args:\n",
        "            text1: First text\n",
        "            text2: Second text\n",
        "\n",
        "        Returns:\n",
        "            Similarity score (0-1)\n",
        "        \"\"\"\n",
        "        # Simple Levenshtein distance-based similarity\n",
        "        from difflib import SequenceMatcher\n",
        "        return SequenceMatcher(None, text1, text2).ratio()\n",
        "\n",
        "    def create_sample_data(self, num_samples=100):\n",
        "        \"\"\"\n",
        "        Create sample data for demonstration purposes.\n",
        "\n",
        "        Args:\n",
        "            num_samples: Number of samples to create\n",
        "\n",
        "        Returns:\n",
        "            Path to the created annotations file\n",
        "        \"\"\"\n",
        "        logger.info(f\"Creating {num_samples} sample data entries...\")\n",
        "\n",
        "        # Create sample data directory\n",
        "        sample_data_dir = os.path.join(self.data_dir, 'sample')\n",
        "        os.makedirs(sample_data_dir, exist_ok=True)\n",
        "\n",
        "        # Sample medical terms\n",
        "        medical_terms = [\n",
        "            \"paracetamol 500mg twice daily\",\n",
        "            \"ibuprofen 400mg three times daily\",\n",
        "            \"amoxicillin 250mg every 8 hours\",\n",
        "            \"metformin 500mg with meals\",\n",
        "            \"lisinopril 10mg once daily\",\n",
        "            \"atorvastatin 20mg at bedtime\",\n",
        "            \"levothyroxine 50mcg in the morning\",\n",
        "            \"albuterol inhaler 2 puffs as needed\",\n",
        "            \"aspirin 81mg daily\",\n",
        "            \"omeprazole 20mg before breakfast\"\n",
        "        ]\n",
        "\n",
        "        # Create CSV file\n",
        "        annotations = []\n",
        "        for i in range(num_samples):\n",
        "            # Generate random text\n",
        "            text = random.choice(medical_terms)\n",
        "\n",
        "            # For demonstration, we'd create actual images here\n",
        "            # Instead, we'll just create placeholder entries\n",
        "            image_path = f\"sample/image_{i:03d}.png\"\n",
        "            annotations.append({\n",
        "                'image_path': image_path,\n",
        "                'text': text\n",
        "            })\n",
        "\n",
        "        # Save annotations\n",
        "        annotations_df = pd.DataFrame(annotations)\n",
        "        annotations_file = os.path.join(self.data_dir, 'sample_annotations.csv')\n",
        "        annotations_df.to_csv(annotations_file, index=False)\n",
        "\n",
        "        logger.info(f\"Sample data created and saved to {annotations_file}\")\n",
        "        return annotations_file\n",
        "\n",
        "    def run_complete_pipeline(self, annotations_file=None, num_samples=100):\n",
        "        \"\"\"\n",
        "        Run the complete pipeline from data creation to evaluation.\n",
        "\n",
        "        Args:\n",
        "            annotations_file: Path to annotations file (optional)\n",
        "            num_samples: Number of samples to create if annotations_file is None\n",
        "\n",
        "        Returns:\n",
        "            Evaluation results\n",
        "        \"\"\"\n",
        "        # Create sample data if annotations file not provided\n",
        "        if annotations_file is None:\n",
        "            annotations_file = self.create_sample_data(num_samples)\n",
        "\n",
        "        # Load Mistral model\n",
        "        self.load_mistral_model()\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = self.create_dataset(annotations_file)\n",
        "\n",
        "        # Split dataset\n",
        "        train_val_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train model\n",
        "        self.train_model(train_val_df)\n",
        "\n",
        "        # Evaluate model\n",
        "        results = self.evaluate_model(test_df)\n",
        "\n",
        "        return results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the OCR system\n",
        "    ocr_system = DoctorHandwritingOCR()\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    results = ocr_system.run_complete_pipeline()\n",
        "\n",
        "    # Recognize text from a new image\n",
        "    image_path = \"./data/new_prescription.png\"\n",
        "    if os.path.exists(image_path):\n",
        "        recognized_text = ocr_system.recognize_text(image_path)\n",
        "        print(\"Recognized text:\")\n",
        "        print(f\"Raw OCR: {recognized_text['raw_ocr']}\")\n",
        "        print(f\"Refined text: {recognized_text['refined_text']}\")"
      ],
      "metadata": {
        "id": "8G-PDJxhYJ7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}